{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578d6b54-c1f2-45f3-9d8a-1d6543461a9f",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Part I - Theoretical\n",
    "## Mahla Entezari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd92c0-6368-4a8f-b14e-6e34d688d1a8",
   "metadata": {},
   "source": [
    "#### a) Explain and compare four different kernels used in Support Vector Machines (SVM) (namely, linear, polynomial, RBF and Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62518406-3592-47e6-a5f6-bafffb65564a",
   "metadata": {},
   "source": [
    "- Linear Kernel: This is the simplest kernel, best used when the data is linearly separable. It computes the dot product of the input vectors.\\\n",
    "Suitable when data is linearly separable and computational efficiency is a concern.\n",
    "\n",
    "- Polynomial Kernel: This kernel is effective for non-linear data transformation. It calculates the similarity between points in higher-dimensional space using a polynomial function.\\\n",
    "Useful for capturing moderate non-linear relationships, but parameter tuning is critical.\n",
    "\n",
    "- RBF (Radial Basis Function) Kernel: This is popular for non-linear separation. It measures similarity to landmarks (or support vectors) using a Gaussian-like function, allowing complex decision boundaries.\\\n",
    "Preferred for complex, non-linear relationships, but careful parameter tuning is necessary.\n",
    "\n",
    "- Sigmoid Kernel: This kernel computes the similarity using a hyperbolic tangent function. It is less commonly used than the others and is useful for neural networks and binary classification tasks.\\\n",
    "Rarely used compared to others, suitable for specific applications requiring non-linear transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87608e37-3837-460a-a5e6-459ec182b8a2",
   "metadata": {},
   "source": [
    "#### b) Compare and contrast CatBoost and LightGBM with XGBoost.\n",
    "#### Discuss the unique features and optimizations of each algorithm, and their impact on performance, especially in handling categorical features and large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fc189-1879-4df7-b66d-b079da8def26",
   "metadata": {},
   "source": [
    "- XGBoost: Efficient gradient boosting with parallel processing, requires categorical feature encoding, optimal for medium to large datasets.\\\n",
    "  Offers parallel and distributed computing, requires categorical feature encoding, optimized for medium to large datasets with strong performance across domains.\\\n",
    "  XGBoost for general efficiency\n",
    "- LightGBM: Fast gradient boosting with native categorical feature support (histogram-based), optimized for large datasets and high-dimensional features.\\\n",
    "  Uses gradient-based, leaf-wise tree growth for efficiency, natively supports categorical features with histogram-based splitting, optimal for large datasets and high-dimensional features.\\\n",
    "   LightGBM for speed and large datasets\n",
    "- CatBoost: Handles categorical features automatically (no encoding needed), uses ordered boosting to reduce overfitting, performs well with categorical data and large datasets.\\\n",
    "  Automatically handles categorical features without preprocessing, uses ordered boosting to prevent overfitting, performs well with complex categorical interactions and large datasets.\\\n",
    "  CatBoost for handling categorical features without preprocessing\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544515a-d998-4f69-abf5-d82301c2c22c",
   "metadata": {},
   "source": [
    "#### c) Compare stratified k-fold cross-validation and regular k-fold cross-validation.\n",
    "#### When do we use stratified k-fold cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791e719-98ba-493a-925e-083b40ff4a31",
   "metadata": {},
   "source": [
    "- Regular k-fold cross-validation: Splits data into k folds randomly, suitable for general model evaluation with large, evenly distributed datasets, but may lead to imbalanced class distributions across folds in classification tasks.\\\n",
    "  Suitable when class balance isn't a concern or in scenarios with large, balanced datasets for general model evaluation.\n",
    "  \n",
    "- Stratified k-fold cross-validation: Ensures each fold retains the same class distribution as the original dataset, crucial for maintaining accuracy in model evaluation, especially with imbalanced class distributions.\\\n",
    "Stratified k-fold is preferred in classification tasks to prevent biased model performance estimates caused by uneven class distributions across folds, improving reliability.\\\n",
    "Widely supported in machine learning libraries, specified as stratified to maintain class proportions across folds during cross-validation.\\\n",
    "Provides more accurate performance metrics by reducing the risk of overestimating model performance on imbalanced data.\\\n",
    "Essential when classes are unevenly distributed or when accurate model assessment across all classes is critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665667f6-f9c9-4fb5-92aa-6103de7fdd96",
   "metadata": {},
   "source": [
    "#### d) Discuss the challenges of validating clustering results in the absence of ground truth labels.\n",
    "#### Explore clustering metrics and their acceptable vales and using domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e54ed3-5c62-4f0f-97bd-a3bebdcbfc32",
   "metadata": {},
   "source": [
    "Validating clustering without labels is tough because there's no clear way to measure accuracy.\n",
    "- Silhouette Score: Higher values mean better clusters (range: -1 to 1).\n",
    "- Davies-Bouldin Index: Lower values mean better clusters.\n",
    "- Dunn Index: Higher values mean better clusters.\n",
    "- Calinski-Harabasz Index: Higher values mean better clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fdc9cd-43a5-4adb-8e79-2fff3e0a2843",
   "metadata": {},
   "source": [
    "#### e) Explain how you would determine the optimal number of clusters (K) for a given dataset.\n",
    "#### Compare different methods, such as the Elbow method, Gap statistic, and Silhouette analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab06fae9-de6c-4a68-82e3-296ddab9b0a1",
   "metadata": {},
   "source": [
    "- Elbow Method: Plot the cost(sum of distances within clusters) for different K values. Look for a bend in the plot where adding more clusters doesn't improve much.\n",
    "- Gap Statistic: Compare the clustering results to random data. The best K is where the difference (gap) between them is largest.\n",
    "- Silhouette Analysis: Measure how similar each point is to its cluster compared to others. Choose the K with the highest average score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d46f46-5f85-479d-b44e-22172c17e632",
   "metadata": {},
   "source": [
    "#### f) Whatâ€™s the impact of the choice of different k in k-fold cross-validation on bias-variance tradeoff in model evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb070ae-48bb-45cf-8377-29c08e2f444d",
   "metadata": {},
   "source": [
    "The choice of k in k-fold cross-validation affects the bias-variance tradeoff in model evaluation.\n",
    "\n",
    "- High k (for example=10, LOOCV): Leads to lower bias since models are trained on nearly all data, but higher variance because validation sets are smaller and more sensitive to data variations. This is more computationally intensive.\n",
    "\n",
    "- Low k (for example=5): Results in higher bias due to smaller training sets, but lower variance as larger validation sets provide more stable estimates.\\\n",
    "It is also less computationally demanding.\\\n",
    "Common Choices: k = 5 or k = 10\n",
    "\n",
    "k=10 balance bias and variance well and are computationally feasible for most datasets. For very small datasets, LOOCV might be preferred to maximize training data usage, despite higher variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5bb98-b251-466e-a677-47a458c728e4",
   "metadata": {},
   "source": [
    "#### g) Describe the out-of-bag (OOB) error estimation in bagging. \n",
    "#### How is it computed, and why is it useful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2119b5-29f9-465e-936b-48645dbbd0b1",
   "metadata": {},
   "source": [
    "Out-of-bag (OOB) error estimation is used in bagging (for example Random Forests) to measure prediction error without a separate validation set.\\\n",
    "Create multiple subsets of the original data by sampling with replacement.\\\n",
    "Train each model on its respective subset.\\\n",
    "Each data point not included in a subset is an OOB sample for that model.\\\n",
    " Predict OOB samples using only the models that did not train on them.\\\n",
    "Compare these predictions to actual values to compute OOB error.\\\n",
    "OOB error is useful because it provides an unbiased performance estimate using all available data, eliminating the need for a separate validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4560ee23-d9a8-4f33-bc1a-3dedc3dea7be",
   "metadata": {},
   "source": [
    "#### h) Explore the application of PCA in the field of genomics.\n",
    "#### Discuss how PCA can be used to identify patterns in high-dimensional genetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ceca73-e69a-4a3d-a8a2-c5c9453be5e5",
   "metadata": {},
   "source": [
    "PCA is used in genomics to simplify complex genetic data.\n",
    "\n",
    "- Reduce Complexity: Genetic data has many variables; PCA reduces these to a few important ones.\n",
    "- Visualize Patterns: It helps visualize genetic similarities and differences in 2D or 3D plots.\n",
    "- Identify Groups: PCA can show population groups based on genetic similarities.\n",
    "- Detect Outliers: It finds unusual patterns or errors in the data.\\\n",
    "Select Important Markers: PCA helps choose the most important genetic markers for further study.\\\n",
    "In short, PCA makes it easier to see and understand patterns in large genetic datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8afcb5-a5bb-466e-93db-70d32096f1b1",
   "metadata": {},
   "source": [
    "#### i) Explore the application of SVM algorithms in anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbd165-ef13-419c-bc8d-abc9b4c453a6",
   "metadata": {},
   "source": [
    "- One-Class SVM: Trains on normal data and identifies outliers as anomalies.\n",
    "- Binary Classification SVM: Distinguishes between normal and known anomalous data.\n",
    "- Hyperplane Separation: Finds the optimal boundary to separate normal data from anomalies.\n",
    "- Kernel Functions: Uses functions like linear or radial basis to handle complex patterns.\n",
    "- Advantages: High accuracy, flexibility with different kernels, and scalability for large datasets.\\\n",
    "Applications include detecting network security breaches, identifying fraudulent transactions, and monitoring industrial equipment for malfunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a949b0-25b0-4e78-af42-dd5d8881dda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "qenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
